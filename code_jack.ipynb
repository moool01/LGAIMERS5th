{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "ROOT_DIR = \"data\"\n",
    "\n",
    "y = pd.read_csv(os.path.join(ROOT_DIR, \"train_y.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = \"data\"\n",
    "# RANDOM_STATE = 110\n",
    "\n",
    "# X_Dam = read_excel_file(os.path.join(ROOT_DIR, \"Dam dispensing.xlsx\"))\n",
    "\n",
    "# X_AutoClave = read_excel_file(\n",
    "#     os.path.join(ROOT_DIR, \"Auto clave.xlsx\")\n",
    "# )\n",
    "\n",
    "# X_Fill1 = read_excel_file(\n",
    "#     os.path.join(ROOT_DIR, \"Fill1 dispensing.xlsx\")\n",
    "# )\n",
    "\n",
    "# X_Fill2 = read_excel_file(\n",
    "#     os.path.join(ROOT_DIR, \"Fill2 dispensing.xlsx\")\n",
    "# )\n",
    "\n",
    "# y = pd.read_csv(os.path.join(ROOT_DIR, \"train_y.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_Dam 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dam dispensing\n",
    "file_name = 'Dam dispensing_header.csv'  # 원본 파일 이름\n",
    "file_path = os.path.join(ROOT_DIR, file_name)\n",
    "X_Dam = pd.read_csv(file_path, low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_Dam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Dam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Dam.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 출력 옵션 설정: 생략 없이 모든 열을 출력\n",
    "# pd.set_option('display.max_rows', None)\n",
    "X_Dam.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 빈 리스트 생성\n",
    "results = []\n",
    "# 각 열의 고유 값 개수를 저장할 빈 리스트 생성\n",
    "unique_counts = []\n",
    "\n",
    "# 각 열의 모든 값이 동일한지 확인\n",
    "for column in X_Dam.columns:\n",
    "    if X_Dam[column].nunique() == 1:\n",
    "        print(f\"All values in column '{column}' are the same.\")\n",
    "        results.append(\"same\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' has different values.\")\n",
    "        results.append(\"different\")\n",
    "    \n",
    "    unique_count = X_Dam[column].nunique()\n",
    "    unique_counts.append(unique_count)\n",
    "\n",
    "# 고유 값 개수를 데이터프레임의 마지막 행에 추가\n",
    "X_Dam.loc['고유 개수'] = unique_counts\n",
    "X_Dam.loc['결과'] = results\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Dam.iloc[-2:]\n",
    "print(last_two_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 행에서 'same' 값을 포함하는 열의 개수를 계산\n",
    "last_row = X_Dam.iloc[-1]\n",
    "same_count = (last_row == 'same').sum()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Number of columns containing 'same' in the last row: {same_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 마지막 행에서 'same' 값이 있는 열을 찾기\n",
    "columns_to_exclude1 = X_Dam.iloc[-1][X_Dam.iloc[-1] == 'same'].index\n",
    "\n",
    "# 마지막에서 두 번째 행에서 값이 0인 열을 찾기\n",
    "columns_to_exclude2 = X_Dam.iloc[-2][X_Dam.iloc[-2] == 0].index\n",
    "\n",
    "# 두 리스트를 합치고 중복을 제거\n",
    "columns_to_exclude = list(set(columns_to_exclude1).union(set(columns_to_exclude2)))\n",
    "\n",
    "# 해당 열을 제외한 새로운 데이터프레임 생성\n",
    "X_Dam = X_Dam.drop(columns=columns_to_exclude)\n",
    "\n",
    "print(X_Dam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 열을 합치기\n",
    "# NaN 값을 0으로 변환\n",
    "X_Dam['WorkMode_Collect Result'] = X_Dam['WorkMode_Collect Result'].fillna(0)\n",
    "X_Dam['Unnamed: 221'] = X_Dam['Unnamed: 221'].fillna(0)\n",
    "\n",
    "X_Dam['WorkMode_Collect Result'] = X_Dam['WorkMode_Collect Result'] + X_Dam['Unnamed: 221']\n",
    "\n",
    "# 'Unnamed: 221' 열 제거\n",
    "X_Dam.drop('Unnamed: 221', axis=1, inplace=True)\n",
    "print(X_Dam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "X_Dam.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Dam = X_Dam.iloc[:-2]\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Dam.iloc[-2:]\n",
    "print(last_two_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_Fill1 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill1 dispensing\n",
    "file_name = 'Fill1 dispensing_header.csv'  # 원본 파일 이름\n",
    "file_path = os.path.join(ROOT_DIR, file_name)\n",
    "X_Fill1 = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_Fill1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 출력 옵션 설정: 생략 없이 모든 열을 출력\n",
    "# pd.set_option('display.max_rows', None)\n",
    "X_Fill1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 빈 리스트 생성\n",
    "results = []\n",
    "# 각 열의 고유 값 개수를 저장할 빈 리스트 생성\n",
    "unique_counts = []\n",
    "\n",
    "# 각 열의 모든 값이 동일한지 확인\n",
    "for column in X_Fill1.columns:\n",
    "    if X_Fill1[column].nunique() == 1:\n",
    "        print(f\"All values in column '{column}' are the same.\")\n",
    "        results.append(\"same\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' has different values.\")\n",
    "        results.append(\"different\")\n",
    "    \n",
    "    unique_count = X_Fill1[column].nunique()\n",
    "    unique_counts.append(unique_count)\n",
    "\n",
    "# 고유 값 개수를 데이터프레임의 마지막 행에 추가\n",
    "X_Fill1.loc['고유 개수'] = unique_counts\n",
    "X_Fill1.loc['결과'] = results\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Fill1.iloc[-2:]\n",
    "print(last_two_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 행에서 'same' 값을 포함하는 열의 개수를 계산\n",
    "last_row = X_Fill1.iloc[-1]\n",
    "same_count = (last_row == 'same').sum()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Number of columns containing 'same' in the last row: {same_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 마지막 행에서 'same' 값이 있는 열을 찾기\n",
    "columns_to_exclude1 = X_Fill1.iloc[-1][X_Fill1.iloc[-1] == 'same'].index\n",
    "\n",
    "# 마지막에서 두 번째 행에서 값이 0인 열을 찾기\n",
    "columns_to_exclude2 = X_Fill1.iloc[-2][X_Fill1.iloc[-2] == 0].index\n",
    "\n",
    "# 두 리스트를 합치고 중복을 제거\n",
    "columns_to_exclude = list(set(columns_to_exclude1).union(set(columns_to_exclude2)))\n",
    "\n",
    "# 해당 열을 제외한 새로운 데이터프레임 생성\n",
    "X_Fill1 = X_Fill1.drop(columns=columns_to_exclude)\n",
    "\n",
    "print(X_Fill1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "X_Fill1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill1 = X_Fill1.iloc[:-2]\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Fill1.iloc[-1:]\n",
    "last_two_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_Fill2 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fill2 dispensing\n",
    "file_name = 'Fill2 dispensing_header.csv'  # 원본 파일 이름\n",
    "file_path = os.path.join(ROOT_DIR, file_name)\n",
    "X_Fill2 = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_Fill2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 출력 옵션 설정: 생략 없이 모든 열을 출력\n",
    "# pd.set_option('display.max_rows', None)\n",
    "X_Fill2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 빈 리스트 생성\n",
    "results = []\n",
    "# 각 열의 고유 값 개수를 저장할 빈 리스트 생성\n",
    "unique_counts = []\n",
    "\n",
    "# 각 열의 모든 값이 동일한지 확인\n",
    "for column in X_Fill2.columns:\n",
    "    if X_Fill2[column].nunique() == 1:\n",
    "        print(f\"All values in column '{column}' are the same.\")\n",
    "        results.append(\"same\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' has different values.\")\n",
    "        results.append(\"different\")\n",
    "    \n",
    "    unique_count = X_Fill2[column].nunique()\n",
    "    unique_counts.append(unique_count)\n",
    "\n",
    "# 고유 값 개수를 데이터프레임의 마지막 행에 추가\n",
    "X_Fill2.loc['고유 개수'] = unique_counts\n",
    "X_Fill2.loc['결과'] = results\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Fill2.iloc[-2:]\n",
    "last_two_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 행에서 'same' 값을 포함하는 열의 개수를 계산\n",
    "last_row = X_Fill2.iloc[-1]\n",
    "same_count = (last_row == 'same').sum()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Number of columns containing 'same' in the last row: {same_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 마지막 행에서 'same' 값이 있는 열을 찾기\n",
    "columns_to_exclude1 = X_Fill2.iloc[-1][X_Fill2.iloc[-1] == 'same'].index\n",
    "\n",
    "# 마지막에서 두 번째 행에서 값이 0인 열을 찾기\n",
    "columns_to_exclude2 = X_Fill2.iloc[-2][X_Fill2.iloc[-2] == 0].index\n",
    "\n",
    "# 두 리스트를 합치고 중복을 제거\n",
    "columns_to_exclude = list(set(columns_to_exclude1).union(set(columns_to_exclude2)))\n",
    "\n",
    "# 해당 열을 제외한 새로운 데이터프레임 생성\n",
    "X_Fill2 = X_Fill2.drop(columns=columns_to_exclude)\n",
    "\n",
    "X_Fill2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "X_Fill2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Fill2 = X_Fill2.iloc[:-2]\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_Fill2.iloc[-1:]\n",
    "last_two_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_AutoClave 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto clave\n",
    "file_name = 'Auto clave_header.csv'  # 원본 파일 이름\n",
    "file_path = os.path.join(ROOT_DIR, file_name)\n",
    "X_AutoClave = pd.read_csv(file_path, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_AutoClave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AutoClave.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AutoClave.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 출력 옵션 설정: 생략 없이 모든 열을 출력\n",
    "# pd.set_option('display.max_rows', None)\n",
    "X_AutoClave.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 빈 리스트 생성\n",
    "results = []\n",
    "# 각 열의 고유 값 개수를 저장할 빈 리스트 생성\n",
    "unique_counts = []\n",
    "\n",
    "# 각 열의 모든 값이 동일한지 확인\n",
    "for column in X_AutoClave.columns:\n",
    "    if X_AutoClave[column].nunique() == 1:\n",
    "        print(f\"All values in column '{column}' are the same.\")\n",
    "        results.append(\"same\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' has different values.\")\n",
    "        results.append(\"different\")\n",
    "    \n",
    "    unique_count = X_AutoClave[column].nunique()\n",
    "    unique_counts.append(unique_count)\n",
    "\n",
    "# 고유 값 개수를 데이터프레임의 마지막 행에 추가\n",
    "X_AutoClave.loc['고유 개수'] = unique_counts\n",
    "X_AutoClave.loc['결과'] = results\n",
    "# 마지막 두 행 출력\n",
    "last_two_rows = X_AutoClave.iloc[-2:]\n",
    "print(last_two_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 행에서 'same' 값을 포함하는 열의 개수를 계산\n",
    "last_row = X_AutoClave.iloc[-1]\n",
    "same_count = (last_row == 'same').sum()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Number of columns containing 'same' in the last row: {same_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 마지막 행에서 'same' 값이 있는 열을 찾기\n",
    "columns_to_exclude1 = X_AutoClave.iloc[-1][X_AutoClave.iloc[-1] == 'same'].index\n",
    "\n",
    "# 마지막에서 두 번째 행에서 값이 0인 열을 찾기\n",
    "columns_to_exclude2 = X_AutoClave.iloc[-2][X_AutoClave.iloc[-2] == 0].index\n",
    "\n",
    "# 두 리스트를 합치고 중복을 제거\n",
    "columns_to_exclude = list(set(columns_to_exclude1).union(set(columns_to_exclude2)))\n",
    "\n",
    "# 해당 열을 제외한 새로운 데이터프레임 생성\n",
    "X_AutoClave = X_AutoClave.drop(columns=columns_to_exclude)\n",
    "\n",
    "X_AutoClave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "X_AutoClave.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "X_Dam.columns = [i + \" - Dam\" for i in X_Dam.columns]\n",
    "X_AutoClave.columns = [i + \" - AutoClave\" for i in X_AutoClave.columns]\n",
    "X_Fill1.columns = [i + \" - Fill1\" for i in X_Fill1.columns]\n",
    "X_Fill2.columns = [i + \" - Fill2\" for i in X_Fill2.columns]\n",
    "\n",
    "X_Dam = X_Dam.rename(columns={\"Set ID - Dam\": \"Set ID\"})\n",
    "X_AutoClave = X_AutoClave.rename(columns={\"Set ID - AutoClave\": \"Set ID\"})\n",
    "X_Fill1 = X_Fill1.rename(columns={\"Set ID - Fill1\": \"Set ID\"})\n",
    "X_Fill2 = X_Fill2.rename(columns={\"Set ID - Fill2\": \"Set ID\"})\n",
    "\n",
    "# Merge X\n",
    "X = pd.merge(X_Dam, X_AutoClave, on=\"Set ID\")\n",
    "X = pd.merge(X, X_Fill1, on=\"Set ID\")\n",
    "X = pd.merge(X, X_Fill2, on=\"Set ID\")\n",
    "X = X.drop(X[X.duplicated(subset=\"Set ID\")].index).reset_index(drop=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X and y\n",
    "df_merged = pd.merge(X, y, \"inner\", on=\"Set ID\")\n",
    "\n",
    "# Drop columns with more than half of the values missing\n",
    "drop_cols = []\n",
    "for column in df_merged.columns:\n",
    "    if (df_merged[column].notnull().sum() // 2) < df_merged[\n",
    "        column\n",
    "    ].isnull().sum():\n",
    "        drop_cols.append(column)\n",
    "df_merged = df_merged.drop(drop_cols, axis=1)\n",
    "\n",
    "# Drop Lot ID\n",
    "df_merged = df_merged.drop(\"LOT ID - Dam\", axis=1)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE =110\n",
    "normal_ratio = 1.0  # 1.0 means 1:1 ratio\n",
    "\n",
    "df_normal = df_merged[df_merged[\"target\"] == \"Normal\"]\n",
    "df_abnormal = df_merged[df_merged[\"target\"] == \"AbNormal\"]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(\n",
    "    n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE\n",
    ")\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.sort_values(by=[\"Collect Date - Dam\"])\n",
    "df_train, df_val = train_test_split(\n",
    "    df_concat,\n",
    "    test_size=0.3,\n",
    "    stratify=df_concat[\"target\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "\n",
    "def print_stats(df: pd.DataFrame):\n",
    "    num_normal = len(df[df[\"target\"] == \"Normal\"])\n",
    "    num_abnormal = len(df[df[\"target\"] == \"AbNormal\"])\n",
    "\n",
    "    print(\n",
    "        f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\"\n",
    "        + f\" ratio: {num_abnormal/num_normal}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "print(f\"  \\tAbnormal\\tNormal\")\n",
    "print_stats(df_train)\n",
    "print_stats(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for col in df_train.columns:\n",
    "    try:\n",
    "        df_train[col] = df_train[col].astype(int)\n",
    "        features.append(col)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if \"Set ID\" in features:\n",
    "    features.remove(\"Set ID\")\n",
    "\n",
    "train_x = df_train[features]\n",
    "train_y = df_train[\"target\"]\n",
    "\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_y = pd.read_csv(os.path.join(\"submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(X, df_test_y, \"inner\", on=\"Set ID\")\n",
    "df_test_x = df_test[features]\n",
    "\n",
    "for col in df_test_x.columns:\n",
    "    try:\n",
    "        df_test_x.loc[:, col] = df_test_x[col].astype(int)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(df_test_x)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming 'target' is the label column in df_val\n",
    "val_y_true = df_val['target']\n",
    "val_pred = model.predict(df_val[features])  # 예측 값\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(val_y_true, val_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
